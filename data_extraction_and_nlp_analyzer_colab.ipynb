{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import main requirements\n",
        "import nltk, shutil\n",
        "shutil.rmtree('/root/nltk_data', ignore_errors=True)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "import time"
      ],
      "metadata": {
        "id": "-HF0qdhn-YaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fde7862-4f82-4630-86ac-46695bc1a19f"
      },
      "id": "-HF0qdhn-YaE",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force clean punkt download if corrupted\n",
        "punkt_path = os.path.join(nltk.data.find(\"tokenizers\"), \"punkt\")\n",
        "if os.path.exists(punkt_path):\n",
        "    shutil.rmtree(punkt_path)\n",
        "\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords', force=True)\n",
        "!rm -rf /root/nltk_data/tokenizers/punkt && python -c \"import nltk; nltk.download('punkt')\""
      ],
      "metadata": {
        "id": "eyx0bERXXIzS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032bc308-1ff4-428b-914b-d5eb1993ccb8"
      },
      "id": "eyx0bERXXIzS",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "06b15a8f",
      "metadata": {
        "id": "06b15a8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "6529c843-f6c8-4466-b671-d6177fa39082"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7d29aac3-1e3c-4f38-9477-80c3a699bfad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7d29aac3-1e3c-4f38-9477-80c3a699bfad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Input.xlsx to Input.xlsx\n",
            "Saving negative-words.txt to negative-words.txt\n",
            "Saving positive-words.txt to positive-words.txt\n"
          ]
        }
      ],
      "source": [
        "# File upload\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5a723601",
      "metadata": {
        "id": "5a723601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea07b60-4ac1-42b8-f82b-acdf5f2dd0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK punkt...\n",
            "Downloading NLTK punkt_tab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK stopwords...\n",
            "Downloading NLTK wordnet...\n",
            "Downloading NLTK averaged_perceptron_tagger...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Netclan20241017...\n",
            "Fetching Netclan20241018...\n",
            "Fetching Netclan20241019...\n",
            "Fetching Netclan20241020...\n",
            "Fetching Netclan20241021...\n",
            "Fetching Netclan20241022...\n",
            "Fetching Netclan20241023...\n",
            "Fetching Netclan20241024...\n",
            "Fetching Netclan20241025...\n",
            "Fetching Netclan20241026...\n",
            "Fetching Netclan20241027...\n",
            "Fetching Netclan20241028...\n",
            "Fetching Netclan20241029...\n",
            "Fetching Netclan20241030...\n",
            "Fetching Netclan20241031...\n",
            "Fetching Netclan20241032...\n",
            "Fetching Netclan20241033...\n",
            "Fetching Netclan20241034...\n",
            "Fetching Netclan20241035...\n",
            "Fetching Netclan20241036...\n",
            "Fetching Netclan20241037...\n",
            "Fetching Netclan20241038...\n",
            "Fetching Netclan20241039...\n",
            "Fetching Netclan20241040...\n",
            "Fetching Netclan20241041...\n",
            "Fetching Netclan20241042...\n",
            "Fetching Netclan20241043...\n",
            "Fetching Netclan20241044...\n",
            "Fetching Netclan20241045...\n",
            "Fetching Netclan20241046...\n",
            "Fetching Netclan20241047...\n",
            "Fetching Netclan20241048...\n",
            "Fetching Netclan20241049...\n",
            "Fetching Netclan20241050...\n",
            "Fetching Netclan20241051...\n",
            "Fetching Netclan20241052...\n",
            "Fetching Netclan20241053...\n",
            "Fetching Netclan20241054...\n",
            "Fetching Netclan20241055...\n",
            "Fetching Netclan20241056...\n",
            "Fetching Netclan20241057...\n",
            "Fetching Netclan20241058...\n",
            "Fetching Netclan20241059...\n",
            "Fetching Netclan20241060...\n",
            "Fetching Netclan20241061...\n",
            "Fetching Netclan20241062...\n",
            "Fetching Netclan20241063...\n",
            "Fetching Netclan20241064...\n",
            "Fetching Netclan20241065...\n",
            "Fetching Netclan20241066...\n",
            "Fetching Netclan20241067...\n",
            "Fetching Netclan20241068...\n",
            "Failed to fetch Netclan20241068: HTTPSConnectionPool(host='insights.blackcoffer.com', port=443): Read timed out. (read timeout=15)\n",
            "Fetching Netclan20241069...\n",
            "Fetching Netclan20241070...\n",
            "Fetching Netclan20241071...\n",
            "Fetching Netclan20241072...\n",
            "Fetching Netclan20241073...\n",
            "Fetching Netclan20241074...\n",
            "Fetching Netclan20241075...\n",
            "Fetching Netclan20241076...\n",
            "Fetching Netclan20241077...\n",
            "Fetching Netclan20241078...\n",
            "Fetching Netclan20241079...\n",
            "Fetching Netclan20241080...\n",
            "Fetching Netclan20241081...\n",
            "Fetching Netclan20241082...\n",
            "Fetching Netclan20241083...\n",
            "Fetching Netclan20241084...\n",
            "Fetching Netclan20241085...\n",
            "Fetching Netclan20241086...\n",
            "Fetching Netclan20241087...\n",
            "Fetching Netclan20241088...\n",
            "Fetching Netclan20241089...\n",
            "Fetching Netclan20241090...\n",
            "Fetching Netclan20241091...\n",
            "Fetching Netclan20241092...\n",
            "Fetching Netclan20241093...\n",
            "Fetching Netclan20241094...\n",
            "Fetching Netclan20241095...\n",
            "Fetching Netclan20241096...\n",
            "Fetching Netclan20241097...\n",
            "Fetching Netclan20241098...\n",
            "Fetching Netclan20241099...\n",
            "Fetching Netclan20241100...\n",
            "Fetching Netclan20241101...\n",
            "Fetching Netclan20241102...\n",
            "Fetching Netclan20241103...\n",
            "Fetching Netclan20241104...\n",
            "Fetching Netclan20241105...\n",
            "Fetching Netclan20241106...\n",
            "Fetching Netclan20241107...\n",
            "Fetching Netclan20241108...\n",
            "Fetching Netclan20241109...\n",
            "Fetching Netclan20241110...\n",
            "Fetching Netclan20241111...\n",
            "Fetching Netclan20241112...\n",
            "Fetching Netclan20241113...\n",
            "Fetching Netclan20241114...\n",
            "Fetching Netclan20241115...\n",
            "Fetching Netclan20241116...\n",
            "Fetching Netclan20241117...\n",
            "Fetching Netclan20241118...\n",
            "Fetching Netclan20241119...\n",
            "Fetching Netclan20241120...\n",
            "Fetching Netclan20241121...\n",
            "Fetching Netclan20241122...\n",
            "Fetching Netclan20241123...\n",
            "Fetching Netclan20241124...\n",
            "Fetching Netclan20241125...\n",
            "Fetching Netclan20241126...\n",
            "Fetching Netclan20241127...\n",
            "Fetching Netclan20241128...\n",
            "Fetching Netclan20241129...\n",
            "Fetching Netclan20241130...\n",
            "Fetching Netclan20241131...\n",
            "Fetching Netclan20241132...\n",
            "Fetching Netclan20241133...\n",
            "Fetching Netclan20241134...\n",
            "Fetching Netclan20241135...\n",
            "Fetching Netclan20241136...\n",
            "Fetching Netclan20241137...\n",
            "Fetching Netclan20241138...\n",
            "Fetching Netclan20241139...\n",
            "Fetching Netclan20241140...\n",
            "Fetching Netclan20241141...\n",
            "Fetching Netclan20241142...\n",
            "Fetching Netclan20241143...\n",
            "Fetching Netclan20241144...\n",
            "Fetching Netclan20241145...\n",
            "Fetching Netclan20241146...\n",
            "Fetching Netclan20241147...\n",
            "Fetching Netclan20241148...\n",
            "Fetching Netclan20241149...\n",
            "Fetching Netclan20241150...\n",
            "Fetching Netclan20241151...\n",
            "Fetching Netclan20241152...\n",
            "Fetching Netclan20241153...\n",
            "Fetching Netclan20241154...\n",
            "Fetching Netclan20241155...\n",
            "Fetching Netclan20241156...\n",
            "Fetching Netclan20241157...\n",
            "Fetching Netclan20241158...\n",
            "Fetching Netclan20241159...\n",
            "Fetching Netclan20241160...\n",
            "Fetching Netclan20241161...\n",
            "Fetching Netclan20241162...\n",
            "Fetching Netclan20241163...\n",
            "Done! File saved as Output.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Ensure all NLTK resources are downloaded\n",
        "def download_nltk_resources():\n",
        "    resources = {\n",
        "        'punkt': ['punkt', 'punkt_tab'],\n",
        "        'stopwords': ['stopwords'],\n",
        "        'wordnet': ['wordnet'],\n",
        "        'averaged_perceptron_tagger': ['averaged_perceptron_tagger']\n",
        "    }\n",
        "\n",
        "    for resource, packages in resources.items():\n",
        "        try:\n",
        "            for package in packages:\n",
        "                try:\n",
        "                    nltk.data.find(f'tokenizers/{package}')\n",
        "                except LookupError:\n",
        "                    print(f\"Downloading NLTK {package}...\")\n",
        "                    nltk.download(package)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading NLTK {resource}: {e}\")\n",
        "\n",
        "# Download required NLTK data\n",
        "download_nltk_resources()\n",
        "\n",
        "def load_wordlist(filepath):\n",
        "    \"\"\"Load word list from file, handling encoding issues.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "        return words\n",
        "    except UnicodeDecodeError:\n",
        "        with open(filepath, 'r', encoding='latin-1') as f:\n",
        "            words = set(line.strip() for line in f if line.strip() and not line.startswith(';'))\n",
        "        return words\n",
        "\n",
        "# Load input file\n",
        "try:\n",
        "    df = pd.read_excel(\"Input.xlsx\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input.xlsx file not found.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Excel file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Validate required columns\n",
        "if not all(col in df.columns for col in ['URL_ID', 'URL']):\n",
        "    print(\"Error: Input.xlsx must contain 'URL_ID' and 'URL' columns.\")\n",
        "    exit()\n",
        "\n",
        "# Load positive and negative words\n",
        "try:\n",
        "    positive_words = load_wordlist(\"positive-words.txt\")\n",
        "    negative_words = load_wordlist(\"negative-words.txt\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading word lists: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Load stopwords from StopWords folder\n",
        "try:\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "except LookupError:\n",
        "    print(\"Downloading NLTK stopwords...\")\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Helper functions\n",
        "def count_syllables(word):\n",
        "    \"\"\"Count syllables in a word with improved accuracy.\"\"\"\n",
        "    word = word.lower().strip(\".:;?!\")\n",
        "    if not word:\n",
        "        return 0\n",
        "\n",
        "    # Handle common exceptions\n",
        "    if word.endswith('es') or word.endswith('ed'):\n",
        "        word = word[:-2]\n",
        "    if word.endswith('le') and len(word) > 2 and word[-3] not in 'aeiouy':\n",
        "        pass\n",
        "\n",
        "    vowels = \"aeiouy\"\n",
        "    count = 0\n",
        "    prev_char_was_vowel = False\n",
        "\n",
        "    # Count vowel groups\n",
        "    for char in word:\n",
        "        if char in vowels:\n",
        "            if not prev_char_was_vowel:\n",
        "                count += 1\n",
        "            prev_char_was_vowel = True\n",
        "        else:\n",
        "            prev_char_was_vowel = False\n",
        "\n",
        "    # Adjust for words ending with 'e'\n",
        "    if word.endswith('e') and count > 1:\n",
        "        count -= 1\n",
        "\n",
        "    return count if count > 0 else 1\n",
        "\n",
        "def count_pronouns(text):\n",
        "    \"\"\"Count personal pronouns with improved regex.\"\"\"\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, flags=re.I)\n",
        "    # Exclude cases where \"US\" might refer to the country\n",
        "    filtered = [p for p in pronouns if (p.lower() != 'us' or\n",
        "               (p == 'us' and not any(word.lower() in ['united', 'states', 'u.s.']\n",
        "                for word in text.split(maxsplit=10))))]\n",
        "    return len(filtered)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean and normalize text.\"\"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Remove unwanted characters\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?\\'\"-]', '', text)\n",
        "    return text\n",
        "\n",
        "# Configure request headers to mimic a real browser\n",
        "# Function to scrape article content\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "    'Accept-Language': 'en-US,en;q=0.5',\n",
        "    'Accept-Encoding': 'gzip, deflate, br',\n",
        "    'Connection': 'keep-alive',\n",
        "    'Upgrade-Insecure-Requests': '1',\n",
        "    'Sec-Fetch-Dest': 'document',\n",
        "    'Sec-Fetch-Mode': 'navigate',\n",
        "    'Sec-Fetch-Site': 'none',\n",
        "    'Sec-Fetch-User': '?1',\n",
        "    'Cache-Control': 'max-age=0',\n",
        "}\n",
        "\n",
        "results = []\n",
        "for _, row in df.iterrows():\n",
        "    try:\n",
        "        url_id, url = row['URL_ID'], row['URL']\n",
        "\n",
        "        # Fetch URL content with error handling\n",
        "        try:\n",
        "            print(f\"Fetching {url_id}...\")\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "\n",
        "            # Check for 406 error specifically\n",
        "            if response.status_code == 406:\n",
        "                # Try alternative headers\n",
        "                alt_headers = headers.copy()\n",
        "                alt_headers['User-Agent'] = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15'\n",
        "                response = requests.get(url, headers=alt_headers, timeout=15)\n",
        "\n",
        "            response.raise_for_status()\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Failed to fetch {url_id}: {e}\")\n",
        "            results.append([url_id, url] + [None] * 13)\n",
        "            continue\n",
        "\n",
        "        # Parse content only if we got HTML\n",
        "        if 'text/html' not in response.headers.get('Content-Type', ''):\n",
        "            print(f\"Non-HTML content for {url_id}\")\n",
        "            results.append([url_id, url] + [None] * 13)\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # Extract text from all paragraph tags\n",
        "        paras = soup.find_all('p')\n",
        "        text = \" \".join(p.get_text().strip() for p in paras if p.get_text().strip())\n",
        "        text = clean_text(text)\n",
        "\n",
        "        if not text:\n",
        "            print(f\"No text extracted from {url_id}\")\n",
        "            results.append([url_id, url] + [None] * 13)\n",
        "            continue\n",
        "\n",
        "        # Tokenization and analysis\n",
        "        try:\n",
        "            sentences = sent_tokenize(text)\n",
        "        except LookupError:\n",
        "            print(\"Downloading NLTK punkt...\")\n",
        "            nltk.download('punkt')\n",
        "            sentences = sent_tokenize(text)\n",
        "\n",
        "        total_sentences = len(sentences)\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "        words = [w for w in tokens if w.isalpha() and w.lower() not in stop_words]\n",
        "        total_words = len(words)\n",
        "\n",
        "        # Function to analyze text\n",
        "        # Sentiment analysis\n",
        "        pos_score = sum(1 for w in words if w.lower() in positive_words)\n",
        "        neg_score = sum(1 for w in words if w.lower() in negative_words)\n",
        "        polarity = (pos_score - neg_score) / ((pos_score + neg_score) + 1e-10)\n",
        "        subjectivity = (pos_score + neg_score) / (total_words + 1e-10)\n",
        "\n",
        "        # Readability metrics\n",
        "        avg_sent_len = total_words / total_sentences if total_sentences else 0\n",
        "        complex_words = [w for w in words if count_syllables(w) > 2]\n",
        "        pct_complex = len(complex_words) / total_words if total_words else 0\n",
        "        fog_index = 0.4 * (avg_sent_len + pct_complex)\n",
        "\n",
        "        # Word metrics\n",
        "        syll_per_word = sum(count_syllables(w) for w in words) / total_words if total_words else 0\n",
        "        avg_word_len = sum(len(w) for w in words) / total_words if total_words else 0\n",
        "\n",
        "        # Personal pronouns\n",
        "        pronoun_count = count_pronouns(text)\n",
        "\n",
        "        results.append([\n",
        "            url_id, url, pos_score, neg_score, polarity, subjectivity,\n",
        "            avg_sent_len, pct_complex * 100, fog_index, avg_sent_len, len(complex_words),\n",
        "            total_words, syll_per_word, pronoun_count, avg_word_len\n",
        "        ])\n",
        "\n",
        "        # Add delay between requests to be polite\n",
        "        time.sleep(2)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {row['URL_ID']}: {e}\")\n",
        "        results.append([url_id, url] + [None] * 13)\n",
        "\n",
        "# Process all URLs and save results\n",
        "cols = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
        "        'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS',\n",
        "        'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT',\n",
        "        'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "# Save to Excel\n",
        "try:\n",
        "    out_df = pd.DataFrame(results, columns=cols)\n",
        "    out_df.to_excel(\"Output.xlsx\", index=False)\n",
        "    print(\"Done! File saved as Output.xlsx\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving results: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7-wiGn-eSI2D",
      "metadata": {
        "id": "7-wiGn-eSI2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "d165f4d1-1b5f-4268-9cef-efe81ad42c45"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_593c59ac-97a4-4924-9d3a-999677c9dfdb\", \"Output.xlsx\", 25716)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Download output file\n",
        "from google.colab import files\n",
        "files.download(\"Output.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bW12wnKCvnjq"
      },
      "id": "bW12wnKCvnjq",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}